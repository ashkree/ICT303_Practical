{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashkree/ICT303_Practical/blob/main/ICT303_Math_Preliminaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDimpLi2_nI"
      },
      "source": [
        "# **ICT303 - Advanced Machine Learning and Artificial Intelligence**\n",
        "# **Lab 2 - Preliminaries**\n",
        "\n",
        "This lab shows how to impolement, using PyTorch, some of the concepts that we covered in the lecture.\n",
        "\n",
        "**Your task:**\n",
        "For each of the concepts defined below, provide\n",
        "- The naive implementation (i.e., without using any pre-build functions of PyTorch or Numpy.\n",
        "- Teh equivalent PyTorch implementation using the concept of tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "CuxhvDlV2sjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Linear Algebra**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RNeECLiQOUgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1. Scalars**\n",
        "A **scalar** is just a number that takes real values. Mathematically, scalars are often denoted using lower-case letters such as $x$, $y$, and $z$.\n",
        "\n",
        "In PyTorch, scales are implemented as tensors that contain only one element."
      ],
      "metadata": {
        "id": "r1LmHB4637af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)"
      ],
      "metadata": {
        "id": "zt4dhUA13dIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply various arithmetic operations on scalars, e.g., addition, subtraction, multiplication, division and exponential operations:"
      ],
      "metadata": {
        "id": "xvdo5xoZ3jKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# addition\n",
        "x + y\n",
        "\n",
        "# multiplication\n",
        "x * y\n",
        "\n",
        "# division\n",
        "x / y\n",
        "\n",
        "# Exponential\n",
        "x**y"
      ],
      "metadata": {
        "id": "1ZV1HSOV3vZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, scalars are referred to as zero-order tensors."
      ],
      "metadata": {
        "id": "s6C6Y7scMY_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2. Vectors**\n",
        "\n",
        "Think of a vector as a 1D array that holds multiple scalar values. For example, if I would like to compute your overal mark in this unit, I need a vector that holds 3 scalar values: your mark in Assignment 1, your mark in Assignment 2, and your mark in the exam.\n",
        "\n",
        "Mathematically, an array is denoted using a bold variable, e.g., **x**. Its elements can be denoted by $x_1, x_2, ...$. It can also be written as:\n",
        "$$\n",
        "\\textbf{x} = \\begin{bmatrix} x_1 \\\\[0.3em] x_2 \\\\[0.3em] \\vdots \\\\[0.3em] x_n \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "In PyTorch, vectors are implemented as first order tensors, i.e., an array that has one dimention:"
      ],
      "metadata": {
        "id": "C0MjJY9i4CQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a tensor that has the values 0, 1 and 2\n",
        "x = torch.arange(3)\n",
        "x"
      ],
      "metadata": {
        "id": "x46Zj4VQ4_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two important properties of vectors that you need to be aware of: length and shape.\n",
        "\n",
        "The length of a vector is the number of elements it contains adn can be accessed using the function\n",
        "\n",
        "```\n",
        "len(x)\n",
        "```\n",
        "It can also be accessed using the function shape that indicates the tensor's length along each axis:\n"
      ],
      "metadata": {
        "id": "yC_lZlks5sP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "YM1UY1X07JU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, vectors are referred to as first-order tensors."
      ],
      "metadata": {
        "id": "-urlUm83MfFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3. Operations on vectors**\n",
        "\n",
        "Write the corresponding Python code for each of the concepts described below. Provide both the naive implementation as well as the one based on PyTorch library functions."
      ],
      "metadata": {
        "id": "KLI3xr8Y7V_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Norm (or length) of a vector**\n",
        "\n",
        "There are two types of norms: the L2 norm (the default one), which is the standard length of a vector and is defined as follows:\n",
        "$$\n",
        "\\| \\textbf{x} \\| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}.\n",
        "$$"
      ],
      "metadata": {
        "id": "a6KO7H5-Ca_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also define the L1 norm of a vector as follows:\n",
        "$$\n",
        "\\| \\textbf{x} \\|_1 = |x_1| + |x_2| + \\cdots + |x_n|.\n",
        "$$\n",
        "\n",
        "Note the use of the subscript $1$ in $\\| \\textbf{x} \\|_1$ for denoting the L1 norm."
      ],
      "metadata": {
        "id": "_BztM7gPCpry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Addition of two vectors**\n",
        "\n",
        "The addition of two vectors  $\n",
        "\\textbf{x} = \\begin{bmatrix} x_1 \\\\[0.3em] x_2 \\\\[0.3em] \\vdots \\\\[0.3em] x_n \\end{bmatrix}$ and $\\textbf{y} = \\begin{bmatrix} y_1 \\\\[0.3em] y_2 \\\\[0.3em] \\vdots \\\\[0.3em] y_n \\end{bmatrix}$  is another vector $\\textbf{z} = \\textbf{x} + \\textbf{y} = \\begin{bmatrix} x_1+y_1 \\\\[0.3em] x_2  + y_2\\\\[0.3em] \\vdots \\\\[0.3em] x_n + y_n\\end{bmatrix}.$"
      ],
      "metadata": {
        "id": "cM4KViJlAcOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiplication of a vector by a scalar**\n",
        "\n",
        "Multiplying a vecor\n",
        "$\n",
        "\\textbf{x} = \\begin{bmatrix} x_1 \\\\[0.3em] x_2 \\\\[0.3em] \\vdots \\\\[0.3em] x_n \\end{bmatrix}$ by a scalar $\\alpha$ results in another vector $\\textbf{z} = \\alpha \\textbf{x} = \\begin{bmatrix} \\alpha x_1 \\\\[0.3em] \\alpha x_2 \\\\[0.3em] \\vdots \\\\[0.3em] \\alpha x_n \\end{bmatrix}$.\n",
        "\n",
        "Note that $\\alpha$ can be any real-valued number (including $0$)."
      ],
      "metadata": {
        "id": "WlsJ4lXaAhfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dot (inner) product of two vectors**\n",
        "\n",
        "The dot, also caller inner, product of otwo vectors  $\n",
        "\\textbf{x} = \\begin{bmatrix} x_1 \\\\[0.3em] x_2 \\\\[0.3em] \\vdots \\\\[0.3em] x_n \\end{bmatrix}$ and $\\textbf{y} = \\begin{bmatrix} y_1 \\\\[0.3em] y_2 \\\\[0.3em] \\vdots \\\\[0.3em] y_n \\end{bmatrix}$, denoted by $\\textbf{x} \\cdot \\textbf{y}$ or $\\langle\\textbf{x}, \\textbf{y}\\rangle$, is a scalar given by:\n",
        "$$\n",
        "  \\langle\\textbf{x}, \\textbf{y}\\rangle = x_1y_1 + x_2y_2 + \\cdots + x_ny_n.\n",
        "$$\n",
        "\n",
        "It is also equivalent to multiplying the transpose of $\\textbf{x}$, denoted by\n",
        "$\\textbf{x}^T$, by $\\textbf{y}$, i.e.,:\n",
        "$$\n",
        "  \\langle\\textbf{x}, \\textbf{y}\\rangle = \\textbf{x}^T \\textbf{y}.\n",
        "$$\n",
        "\n",
        "In 2D or 3D, the dot product can also be defined as follows:\n",
        "\\begin{equation}\n",
        "  \\langle\\textbf{x}, \\textbf{y}\\rangle = \\|\\textbf{x}\\|  \\|\\textbf{y}\\| cos \\theta,\n",
        "\\end{equation}\n",
        "where $\\theta$ is the angle between the two vectors  $\\textbf{x}$ and $\\textbf{y}$.\n",
        "\n",
        "When the norm of the two vectors $\\textbf{x}$ and $\\textbf{y}$ is one then the dot product between them is actually the cosine of the angle between the two vectors."
      ],
      "metadata": {
        "id": "7EgU-EozApH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4. Matrices**\n",
        "\n",
        "Write the corresponding Python code for each of the concepts described below. Provide both the naive implementation as well as the one based on PyTorch library functions."
      ],
      "metadata": {
        "id": "fiLrLOGTBV33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition**\n",
        "\n",
        "A matrix is a 2D array that has $m$ rows and $n$ columns. It is often denoted by bold capital letters, e.g., $\\textbf{X}$ or $\\textbf{A}$:\n",
        "$$\n",
        "  \\textbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
        "              a_{21} & a_{22} & \\cdots & a_{2n}  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              a_{m1} & a_{m2} & \\cdots & a_{mn}  \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Note tha a vector is a particular case of a matrix: it is a matrix that has one column and $m$ elements.\n",
        "\n",
        "Also, a scalar can be seen as a matrix that has one column and one row.\n",
        "\n",
        "It PyTorch, it is represented as a second-order tensor."
      ],
      "metadata": {
        "id": "tFJKHzFnORYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square matrix**\n",
        "\n",
        "A martix is said to be square if its number of rows is equal to the number of columns, i.e., $m=2$."
      ],
      "metadata": {
        "id": "UpHA_lefDDSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identity matrix**\n",
        "\n",
        "An identity matrix, denoted by $\\textbf{I}$, is a square matrix whose diagonal elements are all equal to $1$ and its off-diagonal elements are equal to $0$. In other words:\n",
        "$$\n",
        "\\forall i, a_{ii} = 1\n",
        "$$\n",
        "and\n",
        "$$\n",
        "  a_{ij} = 0 \\text{ if } i \\ne j.\n",
        "$$"
      ],
      "metadata": {
        "id": "ZAGx9rc2DG-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.5. Operations on matrices**\n",
        "\n",
        "Write the corresponding Python code for each of the concepts described below. Provide both the naive implementation as well as the one based on PyTorch library functions.\n",
        "\n",
        "**The Frobenus norm**\n",
        "\n",
        "Similar to vectors, we can also define the norm of a matrix. It is referred to as the Frobenus norm and is defined as follows:\n",
        "$$\n",
        " \\| A \\| = \\sqrt{\\sum_{i, j} a_{ij}^2}.\n",
        "$$"
      ],
      "metadata": {
        "id": "t5EpoT2NC6Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Addition of matrices**\n",
        "\n",
        "Similar to vectors, the sum of two matrices $\\textbf{A}$ and $\\textbf{B}$ of same dimension is another matrix $\\textbf{C}$ of the same dimennsion sunc that\n",
        "$$\n",
        "\\textbf{C} = \\textbf{A} + \\textbf{B} = \\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\n",
        "              a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              a_{m1} + b_{m1} & a_{m2} + b_{m2}& \\cdots & a_{mn} + b_{mn} \\end{bmatrix}.\n",
        "$$"
      ],
      "metadata": {
        "id": "QDYbu_ZoDN8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Product of a scalar with a matrix**\n",
        "\n",
        "The produc of a matrix $\\textbf{A}$ by a scalar $\\alpha$ is defined as:\n",
        "$$\n",
        "  \\alpha \\textbf{A} = \\textbf{A} \\alpha = \\begin{bmatrix} \\alpha a_{11} & \\alpha a_{12} & \\cdots & a_{1n} \\\\\n",
        "              \\alpha a_{21} & \\alpha a_{22} & \\cdots & \\alpha a_{2n}  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\alpha a_{m1} & \\alpha a_{m2} & \\cdots & \\alpha a_{mn}  \\end{bmatrix}.\n",
        "$$"
      ],
      "metadata": {
        "id": "wUzsbjWlDNvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The transpose of a matrix**"
      ],
      "metadata": {
        "id": "eO6P9FZuDNgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transpose of a matrix $\\textbf{A}$ of size $m \\times n$ is another matrix, denoted by $\\textbf{A}^T$ of size $n \\times m$ obtained by exchanging the the matrix's rows and columns. In other words:\n",
        "\n",
        "$$\n",
        "  \\textbf{A} = \\begin{bmatrix} a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n",
        "              a_{12} & a_{22} & \\cdots & a_{m2}  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              a_{1n} & a_{2n} & \\cdots & a_{mn}  \\end{bmatrix}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "07b32y8vVjHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Symmtric matrix**\n",
        "\n",
        "A matrix $\\textbf{A}$ is said symmetric if it is identical to its transpose. In other words, $\\textbf{A} = \\textbf{A}^T$.\n",
        "\n",
        "Note that, for a matrix to be symmetric, it needs first to be a square matrix."
      ],
      "metadata": {
        "id": "f28g6tDTDgkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Antisymmetric matrix**\n",
        "\n",
        "A matrix $\\textbf{A}$ is said antisymmetric if: $\\textbf{A} = -\\textbf{A}^T$. Such matrix is also referred to as skew-symmetric matrix.\n",
        "\n",
        "Note that, for a matrix to be antisymmetric, it needs first to be a square matrix.\n"
      ],
      "metadata": {
        "id": "XvCbf430DkJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix - Vector product (multiplication)**\n",
        "\n",
        "To make it simple, we can treat the $i-$th row of a matrix $\\textbf{A}$ as a horizontal vector $\\textbf{a}_i^T$ (the transpose is there because vectors are vertical, and thus a horizontal vector is the transpose of a vector). Thus, $\\textbf{A}$ can be written as:\n",
        "\n",
        "$$\n",
        "  \\textbf{A} = \\begin{bmatrix} \\textbf{a}_1^T \\\\\n",
        "              \\textbf{a}_2^T  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\textbf{a}_m^T  \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then, the product of the matrix $\\textbf{A}$ with a vector $\\textbf{x}$ of length $m$ is:\n",
        "$$\n",
        "  \\textbf{A}\\textbf{x} = \\begin{bmatrix} \\textbf{a}_1^T\\textbf{x} \\\\\n",
        "              \\textbf{a}_2^T \\textbf{x} \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\textbf{a}_m^T \\textbf{x} \\end{bmatrix} = \\begin{bmatrix} \\langle \\textbf{a}_1, \\textbf{x}\\rangle \\\\\n",
        "              \\langle \\textbf{a}_2, \\textbf{x}\\rangle \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\langle \\textbf{a}_m, \\textbf{x}\\rangle\\end{bmatrix}.\n",
        "$$"
      ],
      "metadata": {
        "id": "qdcSA6G2DneO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Product (multiplication) of two matrices**\n",
        "\n",
        "Similarly, we can compute the product of two matrices $\\textbf{A}$ (of size $m \\times k$) and $\\textbf{B}$ (of size $k \\times n$) by first writing:\n",
        "$$\n",
        "  \\textbf{A} = \\begin{bmatrix} \\textbf{a}_1^T \\\\\n",
        "              \\textbf{a}_2^T  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\textbf{a}_m^T  \\end{bmatrix},\n",
        "  \\textbf{B} = \\begin{bmatrix} \\textbf{b}_1 &  \\textbf{b}_2 &  \\cdots & \\textbf{b}_n\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then, the product of $\\textbf{A}$ and $\\textbf{B}$ is another matrix $\\textbf{C}$ such that:\n",
        "$$\n",
        "\\textbf{C} = \\textbf{A}\\textbf{B} = \\begin{bmatrix} \\textbf{a}_1^T \\\\\n",
        "              \\textbf{a}_2^T  \\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\textbf{a}_m^T  \\end{bmatrix}\n",
        "  \\begin{bmatrix} \\textbf{b}_1 &  \\textbf{b}_2 &  \\cdots, \\textbf{b}_n\\end{bmatrix} = \\begin{bmatrix} \\langle \\textbf{a}_1, \\textbf{b}_1\\rangle  & \\langle \\textbf{a}_1, \\textbf{b}_2\\rangle & \\cdots & \\langle \\textbf{a}_1, \\textbf{b}_n\\rangle   \\\\\n",
        "              \\langle \\textbf{a}_2, \\textbf{b}_1\\rangle & \\langle \\textbf{a}_2, \\textbf{b}_2\\rangle & \\cdots & \\langle \\textbf{a}_2, \\textbf{b}_n\\rangle\\\\\n",
        "              \\\\\n",
        "              \\vdots\n",
        "              \\\\\n",
        "              \\langle \\textbf{a}_m, \\textbf{b}_1\\rangle & \\langle \\textbf{a}_m, \\textbf{b}_2\\rangle & \\cdots & \\langle \\textbf{a}_m, \\textbf{b}_n\\rangle\\end{bmatrix}.\n",
        "$$"
      ],
      "metadata": {
        "id": "pkvf8efHYHh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal matrix**\n",
        "\n",
        "A matrix $\\textbf{A}$ is said orthogonal of the product of $\\textbf{A}$ with its transpose $\\textbf{A}$ results in the identity matrix $\\textbf{I}$. In other words:\n",
        "$$\n",
        "\\textbf{A} \\textbf{A}^T = \\textbf{A}^T \\textbf{A} = \\textbf{I}.\n",
        "$$"
      ],
      "metadata": {
        "id": "ulppREtDDyVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverse of a square matrix**\n",
        "\n",
        "Teh inverse of a sqaure  matrix $\\textbf{A}$ is another square matrix, denoted by $\\textbf{A}^{-1}$ such that\n",
        "$$\n",
        "\\textbf{A} \\textbf{A}^{-1} = \\textbf{A}^{-1} \\textbf{A} = \\textbf{I}.\n",
        "$$"
      ],
      "metadata": {
        "id": "_DT-ykn2hrqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qD4MSWLBD3nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Probability and statistics**"
      ],
      "metadata": {
        "id": "A97bn7UxEAAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1. Basic probability**\n",
        "\n",
        "**Random variables:** A Random variable, denoted by X, is a numerical description of the outcome of an experiment (from Britannica). Examples include:\n",
        "- Daily temperature\n",
        "- Daily share price\n",
        "- Student mark in ICT303\n",
        "\n",
        "A random variable can be:\n",
        "- Discrete, e.g., the species of the animal observed in an image is a random variable that takes values in \\{dog, cat, horse, ant, spider, …\\}\n",
        "- Continuous, e.g., daily temperatures\n",
        "\n",
        "**Probability:** The probability of a random variable $X$ taking a certain value $a$ , i.e., $Pr(X = a)$, also noted $P(X=a)$, is a number between $0$ and $1$ and describe what are the chances (or likelihood) of $X$ being equal to $a$.\n",
        "\n",
        "The probability distribution for a random variable $X$ describes how the probabilities are distributed over the possible values of the random variable.\n",
        "\n",
        "For example, in ICT303, the mark values are between 0 and 100. What is the chance of having your mark equal to 0, 1, 2, …, 100 in this unit? We can, for example, measure these probabilities and plot them as a graph where the $x$ axis corresponds to the range of marks that can be awarded and the $y$ axis corresponds to the likelihood of occurence of each mark.\n",
        "\n",
        "A popular example of probability distributions in the normal, also called Gaussian, distribution. Its function can be expressed analytically as:\n",
        "$$\n",
        "P(X = x) = P(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right),\n",
        "$$\n",
        "where $\\mu$ is the mean, $\\sigma$ is the standard deviation and $\\sigma^2$ is the variance.\n",
        "\n",
        "A Gaussian distribution as the following shape:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "**Joint Probability**\n",
        "\n",
        "Sometimes, we have many random variables. For example, the wheather at any time can be described in terms of temperature, denoted by $X$, and rain fall, denoted by $Y$. Then the probability of having a certain weather is a function of $X$ and $Y$. It is referred to as the joint probability of $X$ and $Y$ and is denoted by $Pr(X, Y)$ or $P(X, Y)$.\n",
        "\n",
        "\n",
        "**Conditional Probability**\n",
        "\n",
        "Sometimes the likelihood of events to occur changes depending on conditions. For example, the following two questions:\n",
        "- What is the probability to rain this afternoon?\n",
        "- What is the probability to rain this afternoon given that it is cloudy?\n",
        "\n",
        "have different answers. The second one is called *conditional* probability because it is a probabilit but conditioned on some observations (\"it is cloudy\" in the example above).\n",
        "\n",
        "Mathematically, the rain event can be denoted by a random variable $X$ and the cloudy event can be denoted by a variable $Y$. Then, the probability to rain this afternoon given that it is cloudy is written as $Pr(X | Y)$ or $P(X | Y)$ and it reads *the probability of $X$ given $Y$* or *the proability of $X$ conditioned on $Y$*.\n",
        "\n",
        "**Independent random variables**\n",
        "\n",
        "Two random variables can be **dependent** on each other. For example, petrol prices and bus ticket prices depend on each other. If the petrol prices go up, then the bus ticket price will go up. In this case, we say petrol prices and bus ticket proces are statistically dependent.\n",
        "\n",
        "Two random variables can be **independent** or **statistically independent**. An example is your final mark in ICT303 and the temperature in New York on the exam day.\n",
        "\n",
        "**Bayes rule**\n",
        "\n",
        "The joint probability of two variables can be written as follows:\n",
        "$$\n",
        "Pr(X, Y) = Pr(X | Y) Pr(Y) = Pr(Y | X) Pr(X).\n",
        "$$\n",
        "\n",
        "The Bayes rule states that:\n",
        "$$\n",
        "  Pr(X | Y) = \\frac{Pr(Y | X) Pr(X)}{Pr(Y)}.\n",
        "$$\n",
        "\n",
        "The Bayes rule is very simple but very powerful. In fact, it is. the basis of Bayesian Machine Learning, which was very popular in 1990 - 2010 (prior to the emergence of Deep Learning). It is also the foundation of what is currently known on Bayesian Deep Learning.\n",
        "\n",
        "An example of applications of the Bayes rule is in spam filtering. Assume that we are given a document composed of words $w_1, \\cdots, w_n$, and we would like to classify it whether it is a *spam* or *not spam*. In other words, we would like to estimate $P(spam | w_1, \\cdots, w_n)$, i.e., the probability of being a spam given the words $w_1, \\cdots, w_n$.\n",
        "\n",
        "Using the Bayes rule, we can write:\n",
        "$$\n",
        " P(spam | w_1, \\cdots, w_n) = \\frac{P(w_1, \\cdots, w_n | spam) P(spam) }{P(w_1, \\cdots, w_n)}\n",
        "$$\n",
        "\n",
        "We can assume that words occur independently of each other given the label of the document (here, label means spam or not spam). Thus, we can write\n",
        "$$\n",
        "  P(w_1, \\cdots, w_n | spam) P(spam) = \\prod_{i=1}^{n}p(w_i | spam).\n",
        "$$\n",
        "\n",
        "Thus, the equation above becomes:\n",
        "$$\n",
        " P(spam | w_1, \\cdots, w_n) = \\frac{P(spam)}{P(w_1, \\cdots, w_n)}\\prod_{i=1}^{n}p(w_i | spam).\n",
        "$$\n",
        "\n",
        "The process of developing a spam filter can then be written as follows;\n",
        "\n",
        "- **Objective:** Given a document (e.g., an email) composed of words $w_1, \\cdots w_n$, we want to estimate the conditional probability $P(space | w_1, \\cdots w_n)$.\n",
        "- **Data collection:**\n",
        " - Collect examples of emails (header, subject, body, meta data) and label each email as *spam* or *non spam*.\n",
        "\n",
        " - **Training:** During the training phase, for each possible word $w_i$,\n",
        "  - measure $P(w_i | spam)$. This is very easy to do: For all the spam emails,  count how many times  $w_i $ appear in spam emails and divide by the total number of words in all spam emails\n",
        "\n",
        "  - measure $P(spam)$. Some statistics show that the current probability of any message being spam is 80%, at the very least. Thus, one can assume $P(spam) = 0.8$. Others use an unbiased assumption, i.e., they assume the $P(spam) = P(non-spam) = 0.5$.\n",
        "\n",
        "- **At runtime:** When you receive an email, measure $P(w_i | spam)$ for each of its words $w_i$ and plug it into the equation above to obtain $P(spam | w_1, \\cdots, w_n)$. If this probability is higher than a predefined threshold, then one can assume that the email is a spam.\n",
        "\n",
        "Note that, in the algorithm above, the term $P(w_1, \\cdots, w_n)$ is a constant and thus ignore.\n"
      ],
      "metadata": {
        "id": "a1SEV5MfEItT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Gradients**"
      ],
      "metadata": {
        "id": "2DpulRy0EXjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1. Definition**\n",
        "\n"
      ],
      "metadata": {
        "id": "DX-MgFcQEd75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2. The chain rule**"
      ],
      "metadata": {
        "id": "Fs2Cacn_EhZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Automatic differentiation**\n",
        "\n",
        "Here we will illustrate how to use automatic differentiation in PyTorch."
      ],
      "metadata": {
        "id": "lT4ty12jEl2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1. A simple example**\n",
        "\n",
        "Consider the function $y=2\\text{x}^T\\text{x}$ where $\\text{x} = (x_1, \\dots, x_n)^T$ i.e.,  a colum vector of   $n$ variables. To start, let's assign to $\\text{x}$ an initial value:\n"
      ],
      "metadata": {
        "id": "vwWKgnS8ntvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(4.0)  # creates a vector x of 4 elements and assign values 0, 1, 2, 3\n",
        "x"
      ],
      "metadata": {
        "id": "6O1aNSuenzbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will indicate that we will need to compute the gradient with respect to the variable $\\textbf{x}$."
      ],
      "metadata": {
        "id": "zQHgMEQvn1WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)  # Alternatively, you can use `x = torch.arange(4.0, requires_grad=True)`\n",
        "x.grad                  # The default value is None"
      ],
      "metadata": {
        "id": "yzkt3rZWn6P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now calculate the function and assign the result to $\\text{y}$."
      ],
      "metadata": {
        "id": "BNsOsf2In8Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ],
      "metadata": {
        "id": "YtQS7H3_n_Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run the code above, it will display the value of $\\text{y}$ as $28$ and the name of the function that will be used to compute the gradient.\n",
        "\n",
        "We can now take the gradient of $\\text{y}$ with respect to $\\text{x}$ by calling its **backward** method, and can access the gradient via $\\text{x}$’s **grad** attribute:\n"
      ],
      "metadata": {
        "id": "hTPhu_GcoALT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()    # Computes the gradient of y\n",
        "\n",
        "# Gradient with respect to x\n",
        "x.grad"
      ],
      "metadata": {
        "id": "wsvxv6F-oEP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can vertify that the gradient computed with autograd is correct. In fact, we already know that the gradient of the function $y=2\\text{x}^\\top\\text{x}$  with respect to $\\text{x}$ should be $4\\text{x}$. Thus, we can now verify that the automatic gradient computation and the expected result are identical:"
      ],
      "metadata": {
        "id": "q8pM_7xZoHOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4 * x  # compares whether the automatically computed gradient (i.e., x.grad) is equal to the manually computed gradient (i.e, 4*x)"
      ],
      "metadata": {
        "id": "DXXnt55moK2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s calculate another function of $\\textbf{x}$ and take its gradient. Note that PyTorch does not automatically reset the gradient buffer when we record a new gradient. Instead the new gradient is added to the already stored gradient. This behavior comes in handy when we want to optimize the sum of multiple objective functions. To reset the gradient buffer, we can call x.grad.zero() as follows:"
      ],
      "metadata": {
        "id": "QW5ZqSA9oL4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()     # Sums the elements of the vector x\n",
        "y.backward()    # Computes gradient with respect to x\n",
        "x.grad"
      ],
      "metadata": {
        "id": "dukleacSoQ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2. Non-scalar variables**\n",
        "If interested (not needed at this stage), please see Section 2.5.4 of https://d2l.ai/chapter_preliminaries/autograd.html."
      ],
      "metadata": {
        "id": "gGB9MasSoTnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3. Gradients and Python Control Flow**\n",
        "So far we reviewed cases where the path from input to output was well-defined via a function such as $\\text{z} = \\text{x}^3$. Programming offers us a lot more freedom in how we compute results. For instance, we can make them depend on auxiliary variables or condition choices on intermediate results.\n",
        "\n",
        "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable.\n",
        "\n",
        "To illustrate this, consider the following code snippet where the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input $a$."
      ],
      "metadata": {
        "id": "qzsObbC8oV6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "metadata": {
        "id": "7qYhcfJ8o5MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we call this function, passing in a random value as input. Since the input is a random variable, we do not know what form the computational graph will take. However, whenever we execute $f(a)$ on a specific input, we realize a specific computational graph and can subsequently run backward."
      ],
      "metadata": {
        "id": "KvvsXTBKoen_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "PD9WmeXsohUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can access the gradient via the attribut grad of **a**"
      ],
      "metadata": {
        "id": "s-znGKK4ohvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad"
      ],
      "metadata": {
        "id": "jdv22xxYooPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic control flow is very common in deep learning. For instance, when processing text, the computational graph depends on the length of the input. In these cases, automatic differentiation becomes vital for statistical modeling since it is impossible to compute the gradient a priori."
      ],
      "metadata": {
        "id": "TUOYRSnkooqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.4. Some exercises**\n",
        "\n",
        "These exercises are from Section 2.5.6 of the textbook.\n",
        "\n",
        "***Question 1.*** After running the function for backpropagation, immediately run it again and see what happens. Why?\n",
        "\n",
        "***Question 2.*** Let $f(x) = sin(x)$. Plot the graph of $f$ and of its derivative. Do not exploit the fact that the derivative is $cos(x)$ but rather use automatic differentiation to get the result. Instead, use automatic derivation."
      ],
      "metadata": {
        "id": "wBlxHcsvlBp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math\n",
        "\n",
        "#https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, steps=25, requires_grad=True)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# plotting y\n",
        "#plt.plot(x.detach(), y.detach())\n",
        "\n",
        "# Note that the method detach creates a tensor that shares storage with tensor that does not require grad.\n",
        "# It detaches the output from the computational graph. So no gradient will be backpropagated along this variable.\n",
        "\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "owLnkgi9lEZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s compute a single-element output. When you call .backward() on a tensor with no arguments, it expects the calling tensor to contain only a single element, as is the case when computing a loss function. Then you can print the gradient with respect to $x$ and also plot its graph"
      ],
      "metadata": {
        "id": "J4Z5ZFbUlIo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = y.sum()\n",
        "out.backward()\n",
        "\n",
        "print(x.grad)\n",
        "plt.plot(x.detach(), x.grad.detach())"
      ],
      "metadata": {
        "id": "rwgmgNjjlKlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}